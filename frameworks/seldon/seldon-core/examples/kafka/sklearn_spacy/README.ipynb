{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seldon Kafka Integration Example with SpaCy Reddit Model\n",
    "\n",
    "In this model we will build upon the code from the Seldon SpaCy NLP example for text classification.\n",
    "\n",
    "You will learn how to deploy the model using the Kafka protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "In your commands:\n",
    "* helm 3.x\n",
    "* kubectl\n",
    "\n",
    "In cluster:\n",
    "* [Install Seldon](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html)\n",
    "* Install Kafka as per the instructions below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your helm repo has access to the strimzi Kafka charts which we'll use to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"strimzi\" has been added to your repositories\r\n"
     ]
    }
   ],
   "source": [
    "!helm repo add strimzi https://strimzi.io/charts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Kafka operator in your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: my-release\r\n",
      "LAST DEPLOYED: Tue Sep 29 08:31:41 2020\r\n",
      "NAMESPACE: default\r\n",
      "STATUS: deployed\r\n",
      "REVISION: 1\r\n",
      "TEST SUITE: None\r\n",
      "NOTES:\r\n",
      "Thank you for installing strimzi-kafka-operator-0.19.0\r\n",
      "\r\n",
      "To create a Kafka cluster refer to the following documentation.\r\n",
      "\r\n",
      "https://strimzi.io/docs/operators/0.19.0/using.html#deploying-cluster-operator-helm-chart-str\r\n"
     ]
    }
   ],
   "source": [
    "!helm install my-release strimzi/strimzi-kafka-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a kafka cluster instantiation with a simple setup as outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kafka-cluster-config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile kafka-cluster-config.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta1\n",
    "kind: Kafka\n",
    "metadata:\n",
    "  name: my-cluster\n",
    "spec:\n",
    "  kafka:\n",
    "    replicas: 1\n",
    "    listeners:\n",
    "      plain: {}\n",
    "      tls: {}\n",
    "      external:\n",
    "        type: nodeport\n",
    "        tls: false\n",
    "    storage:\n",
    "      type: ephemeral\n",
    "    config:\n",
    "      offsets.topic.replication.factor: 1\n",
    "      transaction.state.log.replication.factor: 1\n",
    "      transaction.state.log.min.isr: 1\n",
    "  zookeeper:\n",
    "    replicas: 1\n",
    "    storage:\n",
    "      type: ephemeral\n",
    "  entityOperator:\n",
    "    topicOperator: {}\n",
    "    userOperator: {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafka.kafka.strimzi.io/my-cluster created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f kafka-cluster-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check that kafka was installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-cluster-entity-operator-df58f8b9f-s9wx5               3/3     Running   0          148m\r\n",
      "my-cluster-kafka-0                                       2/2     Running   0          149m\r\n",
      "my-cluster-zookeeper-0                                   1/1     Running   0          149m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods | grep my-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create topics \n",
    "We now need to create the input and output topics for our reddit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing topics.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile topics.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta1\n",
    "kind: KafkaTopic\n",
    "metadata:\n",
    "  name: reddit-classifier-input\n",
    "  labels:\n",
    "    strimzi.io/cluster: \"my-cluster\"\n",
    "spec:\n",
    "  partitions: 2\n",
    "  replicas: 1\n",
    "---\n",
    "apiVersion: kafka.strimzi.io/v1beta1\n",
    "kind: KafkaTopic\n",
    "metadata:\n",
    "  name: reddit-classifier-output\n",
    "  labels:\n",
    "    strimzi.io/cluster: \"my-cluster\"\n",
    "spec:\n",
    "  partitions: 2\n",
    "  replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkatopic.kafka.strimzi.io/reddit-classifier-input created\r\n",
      "kafkatopic.kafka.strimzi.io/reddit-classifier-output created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f topics.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Spacy Sklearn Model\n",
    "\n",
    "To train the spacy sklearn model you can follow the instructions in the SKlearn Spacy Model Example.\n",
    "\n",
    "Alternatively you can just use the image that is saved in the seldon dockerhub with the image `seldonio/reddit-classifier:0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy SpaCy Text Classifier\n",
    "\n",
    "Now we're able to define the YAML that will be used to deploy the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sdep_reddit_kafka.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile sdep_reddit_kafka.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: reddit-kafka\n",
    "spec:\n",
    "  serverType: kafka\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/reddit-classifier:0.1\n",
    "          name: classifier\n",
    "    svcOrchSpec:\n",
    "      env:\n",
    "      - name: KAFKA_BROKER\n",
    "        value: my-cluster-kafka-brokers.default.svc.cluster.local:9092\n",
    "      - name: KAFKA_INPUT_TOPIC\n",
    "        value: reddit-classifier-input\n",
    "      - name: KAFKA_OUTPUT_TOPIC\n",
    "        value: reddit-classifier-output\n",
    "    graph:\n",
    "      name: classifier\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/reddit-kafka configured\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f sdep_reddit_kafka.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that now the model is running as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit-kafka-default-0-classifier-c6ccdd66f-vmf4v        2/2     Running   0          45m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods | grep reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send real time data for stream processing\n",
    "\n",
    "We can now send real time data for stream processing. \n",
    "\n",
    "Below we send a single input with the text \"This is an input\", which will be consumed from the input topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl run --quiet=true -it --rm kafkaconsumer --image=bitnami/kafka:2.6.0 --restart=Never --command -- \\\n",
    "    bash -c \"echo '{\\\"data\\\": {\\\"ndarray\\\": [\\\"This is an input\\\"]}}' \\\n",
    "    | kafka-console-producer.sh --broker-list my-cluster-kafka-external-bootstrap.default:9094 --topic reddit-classifier-input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data processed\n",
    "\n",
    "We now are able to see all the data that has been pushed to the output topic `reddit-classifier-output`.\n",
    "\n",
    "This allows us to see all the inputs that have been processed. We will be listening for 10 seconds to ensure all data is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6758450844706712,0.32415491552932885]]},\"meta\":{}}\n",
      "\n",
      "[2020-09-29 11:45:06,366] ERROR Error processing message, terminating consumer process:  (kafka.tools.ConsoleConsumer$)\n",
      "org.apache.kafka.common.errors.TimeoutException\n",
      "Processed a total of 9 messages\n"
     ]
    }
   ],
   "source": [
    "!kubectl run --quiet=true -it --rm kafkaproducer --image=bitnami/kafka:2.6.0 --restart=Never --command -- \\\n",
    "    kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-external-bootstrap.default:9094 --topic reddit-classifier-output \\\n",
    "        --from-beginning --timeout-ms 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
